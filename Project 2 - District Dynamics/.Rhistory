x<-c(1,2,3,4)
print(x)
total <- 0
for (j in 1:100) {
total < - total + 1
}
print(total)
total <- 0
for (j in 1:100) {
total <- total + 1
}
print(total)
## District Dynamics
# An analysis of the Housing Market
# Data set: Provided by Kelly Ramsay
# Set working directory and load the data
setwd("/Users/juliangriffin/Desktop/District Dynamics")
clean_data <- read.csv("clean_data.csv")
# Q2a - Check the data
head(clean_data)
# Adjust the dataset by applying conditions on LotSize, Sale_price, and Fin_sqft
clean_data_refit <- clean_data[clean_data$LotSize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
# Add a new column 'ppsq' for the logarithm of price per finished square foot
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
# Fit a linear regression model
linear_model <- lm(ppsq ~ log(LotSize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
head(clean_data)
# Adjust the dataset by applying conditions on LotSize, Sale_price, and Fin_sqft
clean_data_refit <- clean_data[clean_data$Lotsize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
linear_model <- lm(ppsq ~ log(LotSize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
linear_model <- lm(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
summary(linear_model)
# Part A. Run a Multiple Linear Regression
head(clean_data)
library(car)
install.packages("car")
library(car)
library(car)
vif_values <- vif(linear_model)
print(vif_values)
X=model.matrix(model)
X=model.matrix(linear_model)
multiColl::CN(X)
X=linear_model.matrix(linear_model)
X = model.matrix(linear_model)
multiColl::CN(X)
install.packages("multiColl")
library(multiColl)
X <- model.matrix(linear_model)
# Calculate the Condition Number
multiColl::CN(X)
install.packages("leaps")  # Install 'leaps' package
no
library(leaps)
all=leaps::regsubsets(ppsq~log(Lotsize)+Sale_date+Year_Built+District,data=df,nvmax=50,method='exhaustive')
all = leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District, data = clean_data, nvmax = 50, method = 'exhaustive')
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District, data = clean_data_refit, nvmax = 50, method = 'exhaustive')
plot(all,scale='adjr2')
clean_data_refit$District <- as.factor(clean_data_refit$District)
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District, data = clean_data_refit, nvmax = 50, method = 'exhaustive')
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District, data = clean_data_refit, nvmax = 50, method = 'exhaustive')
clean_data_refit$District <- as.factor(clean_data_refit$District)
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District, data = clean_data_refit, nvmax = 50, method = 'exhaustive')
plot(all, scale = "adjr2")
# Set working directory and load dataset
setwd("/Users/juliangriffin/Desktop/District Dynamics")
clean_data <- read.csv("clean_data.csv")
# Preview the first few rows of the data
head(clean_data)
# Filter the dataset based on LotSize, Sale_price, and Fin_sqft conditions
clean_data_refit <- clean_data[clean_data$Lotsize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
# Log transformation to calculate price per square foot
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
linear_model <- lm(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
summary(linear_model)
if (!require(leaps)) install.packages("leaps")
library(leaps)
clean_data_refit$District <- as.factor(clean_data_refit$District)
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms,
data = clean_data_refit, nvmax = 50, method = 'exhaustive')
plot(all, scale = "adjr2")
# Residual diagnostics to validate assumptions
par(mfrow = c(2, 2))  # Arrange plots in 2x2 grid
# Q-Q plot for normality of residuals
qqnorm(residuals(linear_model))
qqline(residuals(linear_model), col = "red")
# Histogram of residuals
hist(residuals(linear_model), col = "green", main = "Histogram of Residuals", xlab = "Residuals")
# Residuals vs. fitted values plot
plot(fitted(linear_model), rstudent(linear_model),
main = "Residuals vs Fitted Values", xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
# Calculate leverage and influence measures
hat_values <- hatvalues(linear_model)
high_leverage <- mean(hat_values > (2 * length(coef(linear_model)) / nrow(clean_data_refit)))
cat("High-leverage observations:", high_leverage * 100, "%\n")
cooks_distance <- cooks.distance(linear_model)
influential_points <- mean(cooks_distance > 1)
cat("Proportion of influential points:", influential_points * 100, "%\n")
setwd("/Users/juliangriffin/Desktop/GitHub/R_Projects_Local/R_Projects/Project 2 - District Dynamics")
clean_data <- read.csv("clean_data.csv")
# ----------------------------------------------------------------------------------------
# District Dynamics: Housing Market Analysis
# ----------------------------------------------------------------------------------------
# Author: Julian Griffin
# Purpose: Explore the relationship between song popularity and its characteristics
# Data Set: Provided by Kelly Ramsay
# ----------------------------------------------------------------------------------------
# 1. Data Wrangling & Preparation
# ----------------------------------------------------------------------------------------
# Important Note: Please set your own working directory as the one in the file is my own!
# Set working directory and load dataset
setwd("/Users/juliangriffin/Desktop/GitHub/R_Projects_Local/R_Projects/Project 2 - District Dynamics")
clean_data <- read.csv("clean_data.csv")
# Preview the first few rows of the data
head(clean_data)
## Filter the dataset with the following parameters:
# 1. Lot size greater than 0 sqft
# 2. Sale price greater than or equal to $10 000
# 3. Finished square feet greater than or equal to 500 units
clean_data_refit <- clean_data[clean_data$Lotsize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
# Log transformation to calculate price per square foot
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
# ----------------------------------------------------------------------------------------
# 2. Model Construction
# ----------------------------------------------------------------------------------------
# Fit a multiple linear regression model
linear_model <- lm(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
# Summarize the regression model
summary(linear_model)
# Check for Multicollinearity using Variance Inflation Factor (VIF)
if (!require(car)) install.packages("car")
library(car)
vif_values <- vif(linear_model)
print(vif_values)
# ----------------------------------------------------------------------------------------
# 3. Refining the Model
# ----------------------------------------------------------------------------------------
# Perform all-subsets regression using 'leaps' package
if (!require(leaps)) install.packages("leaps")
library(leaps)
# Convert District to factor variable
clean_data_refit$District <- as.factor(clean_data_refit$District)
# Perform exhaustive search for best subset
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms,
data = clean_data_refit, nvmax = 50, method = 'exhaustive')
# Visualize adjusted R² values for different models
plot(all, scale = "adjr2")
# ----------------------------------------------------------------------------------------
# 4. Outlier & Influence Analysis
# ----------------------------------------------------------------------------------------
# Calculate leverage and influence measures
hat_values <- hatvalues(linear_model)
high_leverage <- mean(hat_values > (2 * length(coef(linear_model)) / nrow(clean_data_refit)))
cat("High-leverage observations:", high_leverage * 100, "%\n")
cooks_distance <- cooks.distance(linear_model)
influential_points <- mean(cooks_distance > 1)
cat("Proportion of influential points:", influential_points * 100, "%\n")
# ----------------------------------------------------------------------------------------
# 5. Key Findings & Model Performance
# ----------------------------------------------------------------------------------------
## Key Insights:
# - The model explains ~21% of the variability in the price per square foot.
# - Significant variables: log(Lotsize), Sale_date, Year_Built, Bdrms.
# - No significant multicollinearity, all predictors are independent.
# ----------------------------------------------------------------------------------------
# 6. Final Thoughts & Recommendations
# ----------------------------------------------------------------------------------------
# Conclusions:
# - The current model provides useful insights, though some predictors (District) could be refined for better precision.
# - The model performs decently with room for further improvement.
# Recommendations:
# 1. Expand the District variable to capture more trends.
# 2. Check model assumptions to ensure regression robustness.
# 3. Explore advanced techniques (e.g., machine learning) to improve predictive accuracy.
# ----------------------------------------------------------------------------------------
# District Dynamics: Housing Market Analysis
# ----------------------------------------------------------------------------------------
# Author: Julian Griffin
# Purpose: Explore the relationship between song popularity and its characteristics
# Data Set: Provided by Kelly Ramsay
# ----------------------------------------------------------------------------------------
# 1. Data Wrangling & Preparation
# ----------------------------------------------------------------------------------------
# Important Note: Please set your own working directory as the one in the file is my own!
# Set working directory and load dataset
setwd("/Users/juliangriffin/Desktop/GitHub/R_Projects_Local/R_Projects/Project 2 - District Dynamics")
clean_data <- read.csv("clean_data.csv")
# Preview the first few rows of the data
head(clean_data)
## Filter the dataset with the following parameters:
# 1. Lot size greater than 0 sqft
# 2. Sale price greater than or equal to $10 000
# 3. Finished square feet greater than or equal to 500 units
clean_data_refit <- clean_data[clean_data$Lotsize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
# Log transformation to calculate price per square foot
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
# ----------------------------------------------------------------------------------------
# 2. Model Construction
# ----------------------------------------------------------------------------------------
# Fit a multiple linear regression model
linear_model <- lm(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
# Summarize the regression model
summary(linear_model)
# Check for Multicollinearity using Variance Inflation Factor (VIF)
if (!require(car)) install.packages("car")
library(car)
vif_values <- vif(linear_model)
print(vif_values)
# ----------------------------------------------------------------------------------------
# 3. Refining the Model
# ----------------------------------------------------------------------------------------
# Perform all-subsets regression using 'leaps' package
if (!require(leaps)) install.packages("leaps")
library(leaps)
# Convert District to factor variable
clean_data_refit$District <- as.factor(clean_data_refit$District)
# Perform exhaustive search for best subset
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms,
data = clean_data_refit, nvmax = 50, method = 'exhaustive')
# Visualize adjusted R² values for different models
plot(all, scale = "adjr2")
# ----------------------------------------------------------------------------------------
# 4. Outlier & Influence Analysis
# ----------------------------------------------------------------------------------------
# Calculate leverage and influence measures
hat_values <- hatvalues(linear_model)
high_leverage <- mean(hat_values > (2 * length(coef(linear_model)) / nrow(clean_data_refit)))
cat("High-leverage observations:", high_leverage * 100, "%\n")
cooks_distance <- cooks.distance(linear_model)
influential_points <- mean(cooks_distance > 1)
cat("Proportion of influential points:", influential_points * 100, "%\n")
# ----------------------------------------------------------------------------------------
# 5. Key Findings & Model Performance
# ----------------------------------------------------------------------------------------
## Key Insights:
# - The model explains ~21% of the variability in the price per square foot.
# - Significant variables: log(Lotsize), Sale_date, Year_Built, Bdrms.
# - No significant multicollinearity, all predictors are independent.
# ----------------------------------------------------------------------------------------
# 6. Final Thoughts & Recommendations
# ----------------------------------------------------------------------------------------
# Conclusions:
# - The current model provides useful insights, though some predictors (District) could be refined for better precision.
# - The model performs decently with room for further improvement.
# Recommendations:
# 1. Expand the District variable to capture more trends.
# 2. Check model assumptions to ensure regression robustness.
# 3. Explore advanced techniques (e.g., machine learning) to improve predictive accuracy.
# ----------------------------------------------------------------------------------------
# District Dynamics: Housing Market Analysis
# ----------------------------------------------------------------------------------------
# Author: Julian Griffin
# Purpose: Explore the relationship between song popularity and its characteristics
# Data Set: Provided by Kelly Ramsay
# ----------------------------------------------------------------------------------------
# 1. Data Wrangling & Preparation
# ----------------------------------------------------------------------------------------
# Important Note: Please set your own working directory as the one in the file is my own!
# Set working directory and load dataset
setwd("/Users/juliangriffin/Desktop/GitHub/R_Projects_Local/R_Projects/Project 2 - District Dynamics")
clean_data <- read.csv("clean_data.csv")
# Preview the first few rows of the data
head(clean_data)
## Filter the dataset with the following parameters:
# 1. Lot size greater than 0 sqft
# 2. Sale price greater than or equal to $10 000
# 3. Finished square feet greater than or equal to 500 units
clean_data_refit <- clean_data[clean_data$Lotsize > 0 &
clean_data$Sale_price >= 10000 &
clean_data$Fin_sqft >= 500, ]
# Log transformation to calculate price per square foot
clean_data_refit$ppsq <- log(clean_data_refit$Sale_price / clean_data_refit$Fin_sqft)
# ----------------------------------------------------------------------------------------
# 2. Model Construction
# ----------------------------------------------------------------------------------------
# Fit a multiple linear regression model
linear_model <- lm(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms, data = clean_data_refit)
# Summarize the regression model
summary(linear_model)
# Check for Multicollinearity using Variance Inflation Factor (VIF)
if (!require(car)) install.packages("car")
library(car)
vif_values <- vif(linear_model)
print(vif_values)
# ----------------------------------------------------------------------------------------
# 3. Refining the Model
# ----------------------------------------------------------------------------------------
# Perform all-subsets regression using 'leaps' package
if (!require(leaps)) install.packages("leaps")
library(leaps)
# Convert District to factor variable
clean_data_refit$District <- as.factor(clean_data_refit$District)
# Perform exhaustive search for best subset
all <- leaps::regsubsets(ppsq ~ log(Lotsize) + Sale_date + Year_Built + District + Bdrms,
data = clean_data_refit, nvmax = 50, method = 'exhaustive')
# Visualize adjusted R² values for different models
plot(all, scale = "adjr2")
# ----------------------------------------------------------------------------------------
# 4. Outlier & Influence Analysis
# ----------------------------------------------------------------------------------------
# Calculate leverage and influence measures
hat_values <- hatvalues(linear_model)
high_leverage <- mean(hat_values > (2 * length(coef(linear_model)) / nrow(clean_data_refit)))
cat("High-leverage observations:", high_leverage * 100, "%\n")
cooks_distance <- cooks.distance(linear_model)
influential_points <- mean(cooks_distance > 1)
cat("Proportion of influential points:", influential_points * 100, "%\n")
# ----------------------------------------------------------------------------------------
# 5. Key Findings & Model Performance
# ----------------------------------------------------------------------------------------
## Key Insights:
# - The model explains ~21% of the variability in the price per square foot.
# - Significant variables: log(Lotsize), Sale_date, Year_Built, Bdrms.
# - No significant multicollinearity, all predictors are independent.
# ----------------------------------------------------------------------------------------
# 6. Final Thoughts & Recommendations
# ----------------------------------------------------------------------------------------
# Conclusions:
# - The current model provides useful insights, though some predictors (District) could be refined for better precision.
# - The model performs decently with room for further improvement.
# Recommendations:
# 1. Expand the District variable to capture more trends.
# 2. Check model assumptions to ensure regression robustness.
# 3. Explore advanced techniques (e.g., machine learning) to improve predictive accuracy.
